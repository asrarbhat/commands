6.824

LECTURE 1
    distributed system: a set of co-opearting computers communicating over network,to get some coherent task done.
    eg. storage for websites or big data computations. or peer to peer file sharing are some of the examples.
    a lot of infrastructure is done this way.

    if you can solve a problem on single computer then always do it . it is easier.

    try everything else before distributed systems, they are complicated.

    distributed systems are used for performance,we want parallelism
    and to tolerate faults.make multiple computers do the same thing and if one fails,another works .

    and some problems are naturally spread across space /physical reason eg banks

    or for security goals. like i don't trust you so your stuff runs there and mine here and we communicate over internet
    hence for insolation.

    most of this course is about peformance and fault tolerance.

    there are multiple parts working concurrently hence comes with a lot of problems hence makes distributed systems hard.

    and failure patterns are unfamiliar.
    we can have partial failures.
    or some part of network is broken.hence partial failures make distributed systems hard.

    it is hard of make performance increase linearly with increase in size of system. it take a lot of careful design to do
    so.

    somes problems have good known solutions whereas some don't

    big websites need distributed systems.

    there is a lot to research and a lot to find and solve.
    it is nice to know distributed systems if you love building systems.

    this course is about infrastructure for applications.

    INFRASTRUCTURES:
            STORAGE  //a lot known
            COMMUNICATION
            COMPUTATION //eg mapReduce 

    we want to simplify the tech/ build abstractions so it becomes easier to run applications

    the dream to build distributed sytems that act like non distibuted system.
    the behaviour is as simple as a single machine.


    IMPLEMENTATION:
        eg remote procedure calls. RPC
           threads //for concurrency
           concurrency control. //eg locks.

    PEFORMANCE:
        we want scalable speedup / scalbility => linear relationship between resources and performance.
        alternative is to improve code and use better algorithms which is more expensive=> to pay programmers.
        hence you have to be careful about the design

        eg web servers are easy to scale up.each server talks to DB and database is the bottleneck.hence hence we need some design
        work there. 

        it is hard of change code for performance then to use more hardware.

    FAULT TOLELRANCE:
        a single computer can run for years without going down.
        but if you have thousands of computers,odds are high that you will have some failed machine everyday.

        something will go wrong everyday
        and there are hundred of switches and wires and things to wrong everytime.

        we have to make fault tolerance part of the design.

        AVAILABLITY:
            we want system to keep operating even when failure happens.eg using replicas.
        RECOVERABILITY:
            if something goes wrong,someone fixes things and it starts working again. eg using logs to recover.
        
        although there is threshold for both.
        good available system is recoverable as well.
        non volatile storage is used for logging state of the system.
        and we use clever ways to make sure performance of non volatile storage is not a problem
        and replication is used for availablity.

        problem with replicas:
            replicas stop acting like replicas and are little different.
    
    CONSISTENCY:
        eg key value service, put(key,value), get(key).with multiple copies of data.
                              using put to one replica and updates the value and before sending to another replica
                              the sender fails hence now we have different data for same key in the replicas hence
                              inconsistant data.

                              hence we need to define get,and put appropriately and what they do.

                              strongly consistent system: get recent put  //very hard to achieve,needs fast communication,might need to read all replicas.

                              weakly consistant system: you might see old put for some time.
                              

        whenver designing replicas make sure their failure probability is independent of each other
        like put them in different racks.or for functioning they shouldn't be dependent on each other strongly.
        put them in different cities or parts of continent so no earthquake issues.but now the commication is slower.

        systems in real life are often very weak.

    
    MAPREDUCE: CASE STUDY

    originally designed by google. around 2004
    for computation on tera bytes of data for indexing for their search engine.
    or for analysing the web to rank pages.
    indexing is like sorting.

    they were using thousands of computers to do so.
    but they needed to code distributed algorithms.

    hence they were looking for a framework for any person to write query
    to this system without caring about distribtion and hence mapreduce
    would take care of it,and all you do is write a map and a reducer.


    the assumption is that 
    data is like
     input 1
     input 2
     input 3
     and map works on each input and it produces a list of key value pair as output.
     eg for wordcount problem key is the word and value is 1
     each map produces an output for each key and value from that input only

     now collect all instances from all maps and produce key and value list.
     and is handed to reduce, where each list is given to different reducer.
     one call to reduce for every key.
      eg for wordcount it would to count the number of items in list.
      hence each reducer will produce a count for that key/word.

    this whole process is called a JOB and each map call or reduce call is called TASK

    Map(K,V)
    key is the file name which is typically ignored 
    and V is value of input file
    for word count split V into words and for each word
    emit(word,"1")
    
    Map is now just a function that anyone can write.
    it needs to know nothing about the distributed sytem hence making it look like one system.

    Reduce(K,V) //the key is the word and value is the list from all instances across all input files.
    emit(len(V))

    like map this also is just a functions.
    
    
    for a longer job you can pipeline a lot of map reduce jobs.

    sometimes the algorithm might not be easy to conform it to map reduce form. after all map and reduce are pure functions.
    you might need to use multiple map reduce and or use other tools as well.

    programmer only sees map and reduce.and doesn't need to care about internals which is cool.

    there are thousands of worker servers and a master server.
    master server commands to worker server to run given map function on given file.
    and worker produces and file locally that it generates using emit() 
    then the workers rearrange these files for reduce phase.each worker agrees on which keys it will run reduce on and every 
    other worker node has to send data to other worker nodes based on that information.hence 
    workers call reduce on all keys that it is alloted to work on.
    and writes in a new file in this distributed file system.where this whole thing acts as one file from all workers.

    google used GFS google file service,which splits file into 64 kb chunks distributed uniformly over cluster.
    hence for map reduce to work data is already in desired form.

    same machines run GFS and map reduce.
    each workers map works on related data stored on it.
    data usually is transfered in gigabit speed in these clusters.

    network throughout has been the bottleneck for these systems.

    map reduce is not just about hadoop.
    shuffle is what happens after map.

    the goal was to make it easy to program by people who have no idea what was going on underneath.

    mapreduce doesn't take advantage of streaming.
    and shuffle is what needs the network communication.
    output of reduce is also stored in GFS.
    hence another round of network used to store output on GFS,since copies needed for fault tolerance and to be stored across machines.

    modern data centers have fast network thoroughput so no need to run GFS and mapper on same machine.hence can be on different machines.


LECTURE 2: RPC and threads
    we are using GO for labs
        good support for threads and locking and convinient remote procedure call package.
        it is type safe and memory safe.
        it is garbage collected.
        combination of threads and garbage collections is important.
        in c++ you can't free memory until last thread stops working
        and the langauge is simpler than c++

    THREADS:
        threads are the main tool to manage concurrency.
        in GO threads are called go routines.
        threads share address space.
        threads have separate stack and pointers.
        since they share address space,in principle they can access each others stack but we don't usually do that.
        we use threads for multicore parallelism.

        I/O concurrency:
            waiting for multiple replies at a time.
            and sending from multiple threads simultanoesly in the network
            threads can share a port bout usually we don't do that,as that would be a bottleneck.

        also to be doing something in background,instead of using that thing in main program and writing 
        checks directly in main program,it should do checks in background.
        like sleep for a second and do something again.

        another line of thinking is to do asynchornous programming/ event driven programming.
            it has a single thread and single loop that waits for anything that triggers something.
        
        threads are usually more linear and convinient to program and with asynchrous cannot be used to
        harness multiple cores.

        to use both parallalism and asynchornous, we can create as many threads as the no. of processors and 
        each using event driven programming.

        thread challenges:
            since they share data. it is sometimes is critical. threads can share a cache.
            it is very easy to get bugs where you share memory.
            eg modifying the shared variable n=n+1 , it is called RACE condition.
            can be solved using locks/mutexes.
            mut.lock()
            n=n+1
            mut.unlock()

            hnce making it atomic.
            lock can be interior to a data structure if we want it to be that and it is a reasonbale strategy.although
            the programmer might use outside as well it he/she is unfamiliar with the internal locks.

            another problem with threads could be co-ordination.
            you want another thread to wait until another thread generates the data.

            we can use condition variables,to kick threads based on conidtion.
            of wait call.

            deadlock is also a problem with threads. and you run into this problem a lot.
            
            it is very difficult to find RACE as problem could happen once in billion. so we use automated tools to find them.

            you can use shared memory or channels for threads to communicate.
            you might not need locks in this way.
            
LECTURE 3: GFS: case study: how to build big storage.
    storage is one of the most important abstraction when it comes to distributed systems.

    and i was a sucessful and was used for a long time in real,world.

    WHY IS IT HARD?
        there are a lot of things to get right.
        we want peformance,since disks are really slow usually
            done using sharding=>splitting data across machines with replication.
        we want fault tolerance:
            using replication.
            and they should be in sync.
            with replication we risk consistency.
        we want conisistency and we pay for it with low performance.

        the ideal behaviour is like as if it is just one server with one copy of data => strong consistency.

    BAD REPLICATION DESIGN:
                we have two servers each with a replicated database and we want to keep them in sync.
                now if you want to update both the databases and one of the update request fails,we get incosistent databases.

    GFS:
        gfs fixes it,we get better but not perfect design
        it was built in 2003

        given a vast data set that cannot be stored on a single disk.
        they had logs,indexes,web pages etc.

        now they wanted to process them quickly using map reduce.
        
        hence they wanted a big and fast storage system that was general and can be used for any application,hence
        it should be reusable

        to do bigness and fastness they needed to split the data into multiple machines.
        even a single file would get split and,
        failure recovery should be automatic.

        GFS was designed to run on a single data center / machine room

        it was for internal use only .

        paper was publised in 2003

        these ideas were not new.

        but it described a system that already built hence makes it a milestone.

        it used a single master.

        google search can get away with inconsitencies and a bank cannot

        MASTER:
            keeps logs for checkpoints.
            even the chunk servers know what chunks of which files they have so no need to store in disk.
            lease expiration is for chunks then next chunk is primary,one where people read and perform computation on.


        READ:
            application/client has a file name and offset in mind and wants to read it from that offset.
            these are send to master,and master return chunk handle and the list of servers to read from.

            client caches these results to avoid asking master same info again and again.

            now client asks the chunk server for that chunk and tells it the offset,
            and chunk server has these files in ordinary linux files with handle as the name of that files
            and returns the data to the client.

            client would try to get chunks from different server on same rack in data house.

        WRITES:

            client wants to append something to a file.

            client asks the master that it wants to append and asks it for last chunk.
            got writing we only need to write to primary.

            if no primary,then you have to update the most up to date chunk ones.

            first step is to find up to date replica chunk=> this is happening in master.

            master has version numbers so it can figure out which one has the up to date.

            each chunk also know the verison number of chunks it has.

            takes the primary,rest being secondary. the master increment teh version number and it sends to secondary
            that here is the primary with this version so update yourself.

            data is send to primary and second and data is stored in temprary files and then primary writes to end and tells all other to do the same.

            once all secondaries reply sucess to primary and primary then replies sucess to client.
            if any of them fails hence primary sends it failed.

            three replicas per chunk by default.

            master was the limitation as there was a single master.
            and load on single master increasesd.
            
