6.824

LECTURE 1
    distributed system: a set of co-opearting computers communicating over network,to get some coherent task done.
    eg. storage for websites or big data computations. or peer to peer file sharing are some of the examples.
    a lot of infrastructure is done this way.

    if you can solve a problem on single computer then always do it . it is easier.

    try everything else before distributed systems, they are complicated.

    distributed systems are used for performance,we want parallelism
    and to tolerate faults.make multiple computers do the same thing and if one fails,another works .

    and some problems are naturally spread across space /physical reason eg banks

    or for security goals. like i don't trust you so your stuff runs there and mine here and we communicate over internet
    hence for insolation.

    most of this course is about peformance and fault tolerance.

    there are multiple parts working concurrently hence comes with a lot of problems hence makes distributed systems hard.

    and failure patterns are unfamiliar.
    we can have partial failures.
    or some part of network is broken.hence partial failures make distributed systems hard.

    it is hard of make performance increase linearly with increase in size of system. it take a lot of careful design to do
    so.

    somes problems have good known solutions whereas some don't

    big websites need distributed systems.

    there is a lot to research and a lot to find and solve.
    it is nice to know distributed systems if you love building systems.

    this course is about infrastructure for applications.

    INFRASTRUCTURES:
            STORAGE  //a lot known
            COMMUNICATION
            COMPUTATION //eg mapReduce 

    we want to simplify the tech/ build abstractions so it becomes easier to run applications

    the dream to build distributed sytems that act like non distibuted system.
    the behaviour is as simple as a single machine.


    IMPLEMENTATION:
        eg remote procedure calls. RPC
           threads //for concurrency
           concurrency control. //eg locks.

    PEFORMANCE:
        we want scalable speedup / scalbility => linear relationship between resources and performance.
        alternative is to improve code and use better algorithms which is more expensive=> to pay programmers.
        hence you have to be careful about the design

        eg web servers are easy to scale up.each server talks to DB and database is the bottleneck.hence hence we need some design
        work there. 

        it is hard of change code for performance then to use more hardware.

    FAULT TOLELRANCE:
        a single computer can run for years without going down.
        but if you have thousands of computers,odds are high that you will have some failed machine everyday.

        something will go wrong everyday
        and there are hundred of switches and wires and things to wrong everytime.

        we have to make fault tolerance part of the design.

        AVAILABLITY:
            we want system to keep operating even when failure happens.eg using replicas.
        RECOVERABILITY:
            if something goes wrong,someone fixes things and it starts working again. eg using logs to recover.
        
        although there is threshold for both.
        good available system is recoverable as well.
        non volatile storage is used for logging state of the system.
        and we use clever ways to make sure performance of non volatile storage is not a problem
        and replication is used for availablity.

        problem with replicas:
            replicas stop acting like replicas and are little different.
    
    CONSISTENCY:
        eg key value service, put(key,value), get(key).with multiple copies of data.
                              using put to one replica and updates the value and before sending to another replica
                              the sender fails hence now we have different data for same key in the replicas hence
                              inconsistant data.

                              hence we need to define get,and put appropriately and what they do.

                              strongly consistent system: get recent put  //very hard to achieve,needs fast communication,might need to read all replicas.

                              weakly consistant system: you might see old put for some time.
                              

        whenver designing replicas make sure their failure probability is independent of each other
        like put them in different racks.or for functioning they shouldn't be dependent on each other strongly.
        put them in different cities or parts of continent so no earthquake issues.but now the commication is slower.

        systems in real life are often very weak.

    
    MAPREDUCE: CASE STUDY

    originally designed by google. around 2004
    for computation on tera bytes of data for indexing for their search engine.
    or for analysing the web to rank pages.
    indexing is like sorting.

    they were using thousands of computers to do so.
    but they needed to code distributed algorithms.

    hence they were looking for a framework for any person to write query
    to this system without caring about distribtion and hence mapreduce
    would take care of it,and all you do is write a map and a reducer.


    the assumption is that 
    data is like
     input 1
     input 2
     input 3
     and map works on each input and it produces a list of key value pair as output.
     eg for wordcount problem key is the word and value is 1
     each map produces an output for each key and value from that input only

     now collect all instances from all maps and produce key and value list.
     and is handed to reduce, where each list is given to different reducer.
     one call to reduce for every key.
      eg for wordcount it would to count the number of items in list.
      hence each reducer will produce a count for that key/word.

    this whole process is called a JOB and each map call or reduce call is called TASK

    Map(K,V)
    key is the file name which is typically ignored 
    and V is value of input file
    for word count split V into words and for each word
    emit(word,"1")
    
    Map is now just a function that anyone can write.
    it needs to know nothing about the distributed sytem hence making it look like one system.

    Reduce(K,V) //the key is the word and value is the list from all instances across all input files.
    emit(len(V))

    like map this also is just a functions.
    
    
    for a longer job you can pipeline a lot of map reduce jobs.

    sometimes the algorithm might not be easy to conform it to map reduce form. after all map and reduce are pure functions.
    you might need to use multiple map reduce and or use other tools as well.

    programmer only sees map and reduce.and doesn't need to care about internals which is cool.

    there are thousands of worker servers and a master server.
    master server commands to worker server to run given map function on given file.
    and worker produces and file locally that it generates using emit() 
    then the workers rearrange these files for reduce phase.each worker agrees on which keys it will run reduce on and every 
    other worker node has to send data to other worker nodes based on that information.hence 
    workers call reduce on all keys that it is alloted to work on.
    and writes in a new file in this distributed file system.where this whole thing acts as one file from all workers.

    google used GFS google file service,which splits file into 64 kb chunks distributed uniformly over cluster.
    hence for map reduce to work data is already in desired form.

    same machines run GFS and map reduce.
    each workers map works on related data stored on it.
    data usually is transfered in gigabit speed in these clusters.

    network throughout has been the bottleneck for these systems.

    map reduce is not just about hadoop.
    shuffle is what happens after map.

    the goal was to make it easy to program by people who have no idea what was going on underneath.

    mapreduce doesn't take advantage of streaming.
    and shuffle is what needs the network communication.
    output of reduce is also stored in GFS.
    hence another round of network used to store output on GFS,since copies needed for fault tolerance and to be stored across machines.

    modern data centers have fast network thoroughput so no need to run GFS and mapper on same machine.hence can be on different machines.


LECTURE 2: RPC and threads
    we are using GO for labs
        good support for threads and locking and convinient remote procedure call package.
        it is type safe and memory safe.
        it is garbage collected.
        combination of threads and garbage collections is important.
        in c++ you can't free memory until last thread stops working
        and the langauge is simpler than c++

    THREADS:
        threads are the main tool to manage concurrency.
        in GO threads are called go routines.
        threads share address space.
        threads have separate stack and pointers.
        since they share address space,in principle they can access each others stack but we don't usually do that.
        we use threads for multicore parallelism.

        I/O concurrency:
            waiting for multiple replies at a time.
            and sending from multiple threads simultanoesly in the network
            threads can share a port bout usually we don't do that,as that would be a bottleneck.

        also to be doing something in background,instead of using that thing in main program and writing 
        checks directly in main program,it should do checks in background.
        like sleep for a second and do something again.

        another line of thinking is to do asynchornous programming/ event driven programming.
            it has a single thread and single loop that waits for anything that triggers something.
        
        threads are usually more linear and convinient to program and with asynchrous cannot be used to
        harness multiple cores.

        to use both parallalism and asynchornous, we can create as many threads as the no. of processors and 
        each using event driven programming.

        thread challenges:
            since they share data. it is sometimes is critical. threads can share a cache.
            it is very easy to get bugs where you share memory.
            eg modifying the shared variable n=n+1 , it is called RACE condition.
            can be solved using locks/mutexes.
            mut.lock()
            n=n+1
            mut.unlock()

            hnce making it atomic.
            lock can be interior to a data structure if we want it to be that and it is a reasonbale strategy.although
            the programmer might use outside as well it he/she is unfamiliar with the internal locks.

            another problem with threads could be co-ordination.
            you want another thread to wait until another thread generates the data.

            we can use condition variables,to kick threads based on conidtion.
            of wait call.

            deadlock is also a problem with threads. and you run into this problem a lot.
            
            it is very difficult to find RACE as problem could happen once in billion. so we use automated tools to find them.

            you can use shared memory or channels for threads to communicate.
            you might not need locks in this way.
            