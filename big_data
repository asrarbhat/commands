introduction:
    hortonworks sandbox:
        
        it is a platform with hadoop and all other related tech installed.
        you can install virualbox,and hortonworks sandbox,select virualbox 
        while installing,it is 15gb.
        open it and it will open in virtualbox,and click on import and start
        like anything else in virtualbox,it has centos inbuilt.
        login: maria_dev password is also the same.to open ambari on port 8888
        and you can do stuff using ambari,without knowing what happens underneath.

        hortonworks merged with cloudera.

    grouplens.org 
        for free datasets
    
    hadoop:
        came in 2006
        maskot of hadoop is a yellow element.

        distributed storage and distributed processing system.
        it is an opensource software platform.
        since it is distributed so ofcourse it needs a cluster to work on.
        it is horizontal scaling.
            horzontal scaling hopefully linear.
        it is not just for batch processing anymore.

        foundation of hadoop is GFS and mapreduce both by google.2003-2004
        hadoop was built by yahoo.

    hadoop ecosystem:
        at bottom is HDFS:
            hadoop distributed file system.
                makes all hard drives look like one and keeps copies of data.
        on top of HDFS we have YARN:
            yet another resource negotiator:
                for the management of processing resources.
        on top of yarn is mapreduce.
            mapreduce is programming metaphor/model for processing.
        pig and hive sit on top of mapreduce:
            hive is more sql like database.
            pig is high level sql like scripts without writing python or java,and gets converted into mapreduce.
        
        apache ambari :
            gui too for cluster view etc so see what is going on,and to analyse the resources
            you can manage cluster using this.
            it sits on top of everything,you can execute queries as well.
        MESOS: alternative for yarn,sits on hdfs.

        SPARK: sits at same level as mapreduce,to run query on data,scala is used
                to do so preferrably. can be used for ML,streaming etc.
        
        TEZ: like spark,more optimal,used with hive,hive with tez is faster.

        apache HBase: nosql database,it is fast and for very large transactions
                      it can expose data stored in your cluster,it is on top of hdfs.
                      can be used for oltp.
        
        apache storm: to process in time streaming data,like from sensors.
                        it is on top of nothing

        oozie:  a way of scheduling jobs in your cluster,when there are many steps.
        zookeper: for co-ordination in cluster which nodes are up etc and which are down
                  keeping track of shared state.
        
        data ingestion:
            getting data into your  system.
                kafaka : collects data from any source
                flume  : for weblogs to your cluster
                sqoop  : tieing your hdfs with relational database
        external data storage:
            mysql
            mongodb
            cassandra

            you can import and export data b/w these and hadoop
        
        query engines:
            like hive but not too closely tighed to hadoop

            hue : for cloudera,it is like ambari of cloudera
            apache drill : for nosql
            presto
            zeppelin
            apache phoenix.


hdfs:
    hadoop distributed file system
    to handle big data:
        broken into blocks of each 128MB blocks.
        split across computers,for parallel processing.
        in order to handle failures,we replicate.
        can run on commodity computers: normal basic computers,to gpu etc
    hdfs architecture:
                        name node
            
            data node      data node       data node

        name node keeps track of files and edit logs and it keeps track of what is
        on data nodes.

        reading:
            client library interacts with name node to get data and figures out where data is stored
            and then gets data from data nodes.

        writing:
            tell the name node
            write to data node  and data nodes talk to each other for replication.
            and acknowledgement sent via client to namenode
            namenode records.

        namenode resilience:
            backup metadata:
                namenode write to local disk and NFS mounts,and we can start new namenode.
            or use secondary namenode
                maintain copy at all time.
                although at a time only one active namenode.
            HDFS federation:
                each namenode mangages a specific namespace volume
                although this is not namenode resilience type.
            
        HDFS high availability:
            hot stadby namenode using shared edit login
            zookeeper trck active namenode.
            uses extreme measures to ensure only one namenode is used at a time.
        

        using hdfs:
            UI : ambari,drag and drop to copy files into and out of hdfs
            command line interface
            HTTP/HDFS proxies
            java interface
            NFS Gateway

        hadoop is written in java underneath the hood.

        using commad line to use hdfs.
            to connect to hortonworks
                maria_dev@127.0.0.1 or anthing using ssh
            
            hadoop fs -ls        to list files.
            hadoop fs -mkdir     first 
            
            you can use ambari to do the same using gui 

            uploading to hdfs
            hadoop fs -copyFromLocal u.data first/u.data
            hadoop fs -rm first/u.data
            hadoop fs -rmdir first
            hadoop fs to see all commands.
            hadoop fs -copyToLocal first/u.data ./
            or hadoop fs -get first/u.data ./
            fs here is generic
            we should use dfs but hadoop dfs is now depricated so use use hdfs dfs 