introduction:
    hortonworks sandbox:
        
        it is a platform with hadoop and all other related tech installed.
        you can install virualbox,and hortonworks sandbox,select virualbox 
        while installing,it is 15gb.
        open it and it will open in virtualbox,and click on import and start
        like anything else in virtualbox,it has centos inbuilt.
        login: maria_dev password is also the same.to open ambari on port 8888
        and you can do stuff using ambari,without knowing what happens underneath.

        hortonworks merged with cloudera.

    grouplens.org 
        for free datasets
    
    hadoop:
        came in 2006
        maskot of hadoop is a yellow elephant.

        distributed storage and distributed processing system.
        it is an opensource software platform.
        since it is distributed so ofcourse it needs a cluster to work on.
        it is horizontal scaling.
            horzontal scaling hopefully linear.
        it is not just for batch processing anymore.

        foundation of hadoop is GFS and mapreduce both by google.2003-2004
        hadoop was built by yahoo.

    hadoop ecosystem:
        at bottom is HDFS:
            hadoop distributed file system.
                makes all hard drives look like one and keeps copies of data.
        on top of HDFS we have YARN:
            yet another resource negotiator:
                for the management of processing resources.
        on top of yarn is mapreduce.
            mapreduce is programming metaphor/model for processing.
        pig and hive sit on top of mapreduce:
            hive is more sql like database.
            pig is high level sql like scripts without writing python or java,and gets converted into mapreduce.
        
        apache ambari :
            gui too for cluster view etc so see what is going on,and to analyse the resources
            you can manage cluster using this.
            it sits on top of everything,you can execute queries as well.
        MESOS: alternative for yarn,sits on hdfs.

        SPARK: sits at same level as mapreduce,to run query on data,scala is used
                to do so preferrably. can be used for ML,streaming etc.
        
        TEZ: like spark,more optimal,used with hive,hive with tez is faster.

        apache HBase: nosql database,it is fast and for very large transactions
                      it can expose data stored in your cluster,it is on top of hdfs.
                      can be used for oltp.
        
        apache storm: to process in time streaming data,like from sensors.
                        it is on top of nothing

        oozie:  a way of scheduling jobs in your cluster,when there are many steps.
        zookeper: for co-ordination in cluster which nodes are up etc and which are down
                  keeping track of shared state.
        
        data ingestion:
            getting data into your  system.
                kafaka : collects data from any source
                flume  : for weblogs to your cluster
                sqoop  : tieing your hdfs with relational database
        external data storage:
            mysql
            mongodb
            cassandra

            you can import and export data b/w these and hadoop
        
        query engines:
            like hive but not too closely tighed to hadoop

            hue : for cloudera,it is like ambari of cloudera
            apache drill : for nosql
            presto
            zeppelin
            apache phoenix.


hdfs:
    hadoop distributed file system
    to handle big data:
        broken into blocks of each 128MB blocks.
        split across computers,for parallel processing.
        in order to handle failures,we replicate.
        can run on commodity computers: normal basic computers,to gpu etc
    hdfs architecture:
                        name node
            
            data node      data node       data node

        name node keeps track of files and edit logs and it keeps track of what is
        on data nodes.

        reading:
            client library interacts with name node to get data and figures out where data is stored
            and then gets data from data nodes.

        writing:
            tell the name node
            write to data node  and data nodes talk to each other for replication.
            and acknowledgement sent via client to namenode
            namenode records.

        namenode resilience:
            backup metadata:
                namenode write to local disk and NFS mounts,and we can start new namenode.
            or use secondary namenode
                maintain copy at all time.
                although at a time only one active namenode.
            HDFS federation:
                each namenode mangages a specific namespace volume
                although this is not namenode resilience type.
            
        HDFS high availability:
            hot stadby namenode using shared edit login
            zookeeper trck active namenode.
            uses extreme measures to ensure only one namenode is used at a time.
        

        using hdfs:
            UI : ambari,drag and drop to copy files into and out of hdfs
            command line interface
            HTTP/HDFS proxies
            java interface
            NFS Gateway

        hadoop is written in java underneath the hood.

        using commad line to use hdfs.
            to connect to hortonworks
                maria_dev@127.0.0.1 or anthing using ssh
            
            hadoop fs -ls        to list files.
            hadoop fs -mkdir     first 
            
            you can use ambari to do the same using gui 

            uploading to hdfs
            hadoop fs -copyFromLocal u.data first/u.data
            hadoop fs -rm first/u.data
            hadoop fs -rmdir first
            hadoop fs to see all commands.
            hadoop fs -copyToLocal first/u.data ./
            or hadoop fs -get first/u.data ./
            fs here is generic
            we should use dfs but hadoop dfs is now depricated so use use hdfs dfs introduction

mapreduce:
    one of the core pieces of hadoop along with hdfs and yarn.

    distributes the processing of data on your cluster.

    divides your data up into partitions that are mapped/transformed and Reduced/aggregated by mapper and reducer functions you define.

    an application master monitors your mapper and reducers on each partitions,hence resilient to failure.
    conceptual model:
        MAPPER converts raw asource data into key/value pairs.
        data => mapper => k1:v k2:v k1:v
            keys can be duplicate.
        next step is shuffle and sort.
            output is same key but value now is list of values.
        now the REDUCER processes each key and does something on it.

        RAW INPUT -> MAPPER -> KEY/VALUE -> SHUFFLE AND SORT -> KEY/VALUE -> REDUCER -> KEY/VALUE

    how mapreduce works underneath:
        mappers run of different nodes in the cluster,each mapper processing a partition of data.
        for shuffle and sort,cluster has to communicate.
        reducers also run of cluster and process a given set of keys.
        final result is also communicated buy whole cluster if it is to be written,and copies to be made.

    ANATOMY OF A MAPREDUCE JOB:
        client node asks YARN
        YARN talkes to Node manager of each node.and keep eye on each node.
        node manager also gets data from hdfs.
    
    MapReduce is natively written in java
    Streaming allows interfacing to other languages (i.e Python)

            MapTask/ReduceTask -> key/values -> streaming Process -> output back to MapTask/ReduceTask.
    
    Handling failure:
        Application master monitors worker tasks for errors or hanging
            restarts as needed
            preferably on a different node.
        
        what if the application mster goes down:
            yarn can try to restart it.
        
        what if an entire Node goes down:
            this could be the application master
            the resource mamager will try to restart it
        
        what if the resource manager does down:
            can set up high availability using zookeeper to have a hot standby.
        
    sometimes it can be hard to conform your problem to map reduce.

writing the mapper using python.

from mrjob.job import MRJob   //abstracks away streaming,mr means mapreduce. 
from mrjob.step import MRStep

class RatingsBreakdown(MRJob):

    def steps(self):
        return [
        MRStep(mapper=self.mapper_get_ratings,
               reducer=self.reducer_count_ratings)
        ]
    def mapper_get_ratings(self,_,line):           //  _ is a key
        userID,movieID,rating,timestamp=line.split('\t')
        yield rating,1
    
    def reducer_count_ratings(self,key,values):
        yield key,sum(values)
    
if __name__=="__main__":
    RatingsBreakdown.run()



installing python,mrjob:
    pip3 install mrjob
    you can install it on sandbox as well.
    mrjob is for streaming to run python code.
to run the job:
    to run locally for checking:
        python3 RatingsBreakdown.py input.txt  //test on small data
    to run on hadoop:
        python3 RatingsBreakdown.py -r hadoop --hadoop-streaming.jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-streaming.jar input.txt or hdfs://data/input.data

        if you give local file,it gets copied to hdfs first.
    
sort and shuffle can be used for sorting data.

    multi-stage jobs:
        you can chain mapreduce:
            def steps(self):
                return [
                    MRStep(mapper=self.mapper_get_ratings),
                    reducer=self.reducer_count_ratings),

                    MRStep(reducer=self.reducer_sorted_output)
                ]
            

"abc".zfill(5) in python fills zeros at the begining to make length 5